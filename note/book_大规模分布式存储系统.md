---
title     : 《大规模分布式存储系统》 读书笔记
date      : 2015-08-07
---

## 1. 概述


## 2. 单机存储引擎
### 脉络
  - 哈希存储引擎  Bitcask
  - B树存储引擎   MySQL InnoDB
  - LSM（Log Structured Merge Tree）树存储引擎  Google Bigtable、Google LevelDB、Cassandra

### 批注
LSM树的优势在于有效地规避了磁盘随机写入问题，但读取时可能需要访问较多的磁盘文件。

1977年，以色列人Jacob Ziv和Abraham Lempel发表论文《顺序数据压缩的一个通用算法》，从此，
LZ系列压缩算法几乎垄断了通用无损压缩领域，常用的Gzip算法中使用的LZ77，GIF图片格式中使用的LZW，
以及LZO等压缩算法都属于这个系列。


## 3. 分布式系统
### 脉络
  - 分布（数据分布均匀，负载均衡）
  - 可靠（副本、复制，一致性）
  - 性能（吞吐和延时估算）
  - 容错（故障类型、检测和恢复）
  - 扩展（总控模型、P2P）


## 4. 分布式文件系统
### 脉络
  GFS 案例
   - 租约：  主副本获取写授权租约，控制多个副本的写入
   - 一致性：追加失败重试，可能出现重复
   - 追加流程：并发顺序，机架位置部署影响，流水线复制
   - 容错：Master 选主、实时热备；chunk server 校验和

### 批注
追加的一致性。如果不发生异常，追加成功的记录在GFS的各个副本中是确定并且严格一致的；
但是如果出现了异常，可能出现某些副本追加成功而某些副本没有成功的情况，失败的副本可能会出现一些可识别的填充（padding）记录。
GFS客户端追加失败将重试，只要返回用户追加成功，说明在所有副本中都至少追加成功了一次。
当然，可能出现记录在某些副本中被追加了多次，即重复记录；也可能出现一些可识别的填充记录，应用层需要能够处理这些问题。

GFS Master的修改操作总是先记录操作日志，然后修改内存。当Master发生故障重启时，可以通过磁盘中的操作日志恢复内存数据结构。
另外，为了减少Master宕机恢复时间，Master会定期将内存中的数据以checkpoint文件的形式转储到磁盘中，
从而减少回放的日志量。为了进一步提高Master的可靠性和可用性，GFS中还会执行实时热备，
所有的元数据修改操作都必须保证发送到实时热备才算成功。远程的实时热备将实时接收Master发送的操作日志并在内存中回放这些元数据操作。
如果Master宕机，还可以秒级切换到实时备机继续提供服务。为了保证同一时刻只有一台Master,GFS依赖Google内部的Chubby服务进行选主操作。

当Master创建了一个chunk，它会根据如下因素来选择chunk副本的初始位置：
  1）新副本所在的ChunkServer的磁盘利用率低于平均水平；
  2）限制每个Chunk-Server“最近”创建的数量；
  3）每个chunk的所有副本不能在同一个机架
